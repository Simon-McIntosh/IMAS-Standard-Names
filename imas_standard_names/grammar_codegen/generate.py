"""Generate code (types + metadata) from ``grammar.yml`` and ``tags.yml``.

Outputs:
- ``imas_standard_names/grammar/model_types.py``
- ``imas_standard_names/grammar/tag_types.py``

Lives outside the ``grammar`` package to avoid importing any module that
depends on the generated file during generation.

Note: Generated files are automatically formatted by ruff (check --fix and format)
to ensure consistency with the project's formatting standards. This ensures
pre-commit hooks won't find formatting issues in generated code.
"""

from __future__ import annotations

import subprocess
from collections.abc import Mapping
from pathlib import Path
from textwrap import dedent, indent
from typing import Any

import yaml

from imas_standard_names.grammar_codegen.entry_schema_spec import EntrySchemaSpec
from imas_standard_names.grammar_codegen.spec import GrammarSpec, IncludeLoader
from imas_standard_names.grammar_codegen.tag_spec import TagSpec

HEADER = (
    "Auto-generated grammar models.\n\n"
    "This file was generated by "
    "imas_standard_names.grammar_codegen.generate\n"
    "from imas_standard_names/grammar/specification.yml. Do not edit manually."
)

TAG_HEADER = (
    "Auto-generated tag models.\n\n"
    "This file was generated by "
    "imas_standard_names.grammar_codegen.generate\n"
    "from imas_standard_names/grammar/vocabularies/tags.yml. Do not edit manually."
)

FIELD_SCHEMAS_HEADER = (
    "Auto-generated catalog entry field schemas.\n\n"
    "This file was generated by "
    "imas_standard_names.grammar_codegen.generate\n"
    "from imas_standard_names/grammar/specification.yml (entry_schema section).\n"
    "Do not edit manually."
)

# The output is next to the runtime grammar modules
# Use file path instead of package import to avoid circular dependency
_GRAMMAR_DIR = Path(__file__).resolve().parents[1] / "grammar"
OUTPUT_MODULE = _GRAMMAR_DIR / "model_types.py"
CONSTANTS_OUTPUT_MODULE = _GRAMMAR_DIR / "constants.py"
TAG_OUTPUT_MODULE = _GRAMMAR_DIR / "tag_types.py"
FIELD_SCHEMAS_OUTPUT_MODULE = _GRAMMAR_DIR / "field_schemas.py"

ENUM_NAME_OVERRIDES = {
    "components": "Component",
    "subjects": "Subject",
    "basis": "Basis",
    "geometric_bases": "GeometricBase",
    "generic_physical_bases": "GenericPhysicalBase",
    "objects": "Object",
    "positions": "Position",
    "processes": "Process",
}


def main() -> None:
    # Generate grammar types and constants
    spec = GrammarSpec.load()

    # Generate types.py (enums only)
    types_content = render_types_module(spec)
    types_existing = (
        OUTPUT_MODULE.read_text(encoding="utf-8") if OUTPUT_MODULE.exists() else ""
    )
    types_updated = False
    if types_existing != types_content:
        OUTPUT_MODULE.parent.mkdir(parents=True, exist_ok=True)
        OUTPUT_MODULE.write_text(types_content, encoding="utf-8")
        print("Updated grammar/model_types.py")
        types_updated = True
    else:
        print("grammar/model_types.py already up to date")

    # Generate constants.py (all constants)
    constants_content = render_constants_module(spec)
    constants_existing = (
        CONSTANTS_OUTPUT_MODULE.read_text(encoding="utf-8")
        if CONSTANTS_OUTPUT_MODULE.exists()
        else ""
    )
    constants_updated = False
    if constants_existing != constants_content:
        CONSTANTS_OUTPUT_MODULE.parent.mkdir(parents=True, exist_ok=True)
        CONSTANTS_OUTPUT_MODULE.write_text(constants_content, encoding="utf-8")
        print("Updated grammar/constants.py")
        constants_updated = True
    else:
        print("grammar/constants.py already up to date")

    grammar_updated = types_updated or constants_updated

    # Generate tag types
    tag_spec = TagSpec.load()
    tag_content = render_tag_module(tag_spec)
    tag_existing = (
        TAG_OUTPUT_MODULE.read_text(encoding="utf-8")
        if TAG_OUTPUT_MODULE.exists()
        else ""
    )
    tags_updated = False
    if tag_existing != tag_content:
        TAG_OUTPUT_MODULE.parent.mkdir(parents=True, exist_ok=True)
        TAG_OUTPUT_MODULE.write_text(tag_content, encoding="utf-8")
        print("Updated grammar/tag_types.py")
        tags_updated = True
    else:
        print("grammar/tag_types.py already up to date")

    # Generate field_schemas.py from entry_schema section
    entry_schema_spec = EntrySchemaSpec.load()
    field_schemas_content = render_field_schemas_module(entry_schema_spec)
    field_schemas_existing = (
        FIELD_SCHEMAS_OUTPUT_MODULE.read_text(encoding="utf-8")
        if FIELD_SCHEMAS_OUTPUT_MODULE.exists()
        else ""
    )
    field_schemas_updated = False
    if field_schemas_existing != field_schemas_content:
        FIELD_SCHEMAS_OUTPUT_MODULE.parent.mkdir(parents=True, exist_ok=True)
        FIELD_SCHEMAS_OUTPUT_MODULE.write_text(field_schemas_content, encoding="utf-8")
        print("Updated grammar/field_schemas.py")
        field_schemas_updated = True
    else:
        print("grammar/field_schemas.py already up to date")

    if not grammar_updated and not tags_updated and not field_schemas_updated:
        print("All generated files up to date")
    else:
        # Run ruff --fix on generated files to ensure they pass formatting checks
        generated_files = []
        if types_updated:
            generated_files.append(str(OUTPUT_MODULE))
        if constants_updated:
            generated_files.append(str(CONSTANTS_OUTPUT_MODULE))
        if tags_updated:
            generated_files.append(str(TAG_OUTPUT_MODULE))
        if field_schemas_updated:
            generated_files.append(str(FIELD_SCHEMAS_OUTPUT_MODULE))

        if generated_files:
            print("Running ruff formatting on generated files...")
            project_root = OUTPUT_MODULE.parent.parent.parent
            files_str = ", ".join(
                str(Path(f).relative_to(project_root)) for f in generated_files
            )
            print(f"  Formatting: {files_str}")

            # Run ruff check --fix first (same as pre-commit)
            print("  Running ruff check --fix...")
            result = subprocess.run(
                ["uv", "run", "ruff", "check", "--fix"] + generated_files,
                capture_output=True,
                text=True,
                cwd=project_root,
            )
            if result.returncode != 0:
                print("✗ Ruff check --fix failed:")
                if result.stdout:
                    print(result.stdout)
                if result.stderr:
                    print(result.stderr)
                raise RuntimeError(
                    f"ruff check --fix failed with return code {result.returncode}. "
                    "Generated files must pass ruff formatting checks."
                )
            if result.stdout.strip():
                print(f"  {result.stdout.strip()}")

            # Run ruff format to ensure proper formatting (same as pre-commit)
            print("  Running ruff format...")
            result = subprocess.run(
                ["uv", "run", "ruff", "format"] + generated_files,
                capture_output=True,
                text=True,
                cwd=project_root,
            )
            if result.returncode != 0:
                print("✗ Ruff format failed:")
                if result.stdout:
                    print(result.stdout)
                if result.stderr:
                    print(result.stderr)
                raise RuntimeError(
                    f"ruff format failed with return code {result.returncode}. "
                    "Generated files must pass ruff formatting checks."
                )
            if result.stdout.strip():
                print(f"  {result.stdout.strip()}")

            print("✓ All generated files formatted successfully")


def render_types_module(spec: Any) -> str:
    """Render types.py with only enum definitions."""
    sections = [
        _module_header(),
        _types_import_block(),
        _enum_definitions(spec),
        _types_export_block(spec),
    ]

    # Join sections with proper spacing
    parts = []
    for section in sections:
        if not section:
            continue
        text = section.strip()
        if not text:
            continue

        if parts:
            # Add exactly one blank line between sections
            parts.append("\n\n")
        parts.append(text)

    body = "".join(parts)
    return f"{body}\n"


def render_constants_module(spec: Any) -> str:
    """Render constants.py with all metadata constants."""
    metadata = _segment_metadata(spec)
    sections = [
        _constants_header(),
        _constants_import_block(spec),
        _segment_rule_dataclass(),
        _render_segment_metadata(metadata),
        _render_scope_metadata(spec),
        _constants_export_block(),
    ]

    parts: list[str] = []
    for section in sections:
        if not section:
            continue
        text = section.strip()
        if not text:
            continue
        if parts:
            parts.append("\n\n")
        parts.append(text)

    body = "".join(parts)
    return f"{body}\n"


def _module_header() -> str:
    return f'"""Auto-generated grammar models.\n\n{HEADER}"""'


def _constants_header() -> str:
    return f'"""Grammar constants and metadata.\n\nThis module contains all constants and metadata used by the grammar system,\nseparated from the types to avoid circular imports between grammar/support.py\nand grammar/model_types.py.\n\n{HEADER}"""'


def _types_import_block() -> str:
    return (
        dedent(
            """
from __future__ import annotations

from enum import StrEnum

        """
        ).strip()
        + "\n"
    )


def _constants_import_block(spec: Any) -> str:
    # Only import enums that are used in segment token map
    used_enums = set()
    for segment in spec.segments:
        if segment.vocabulary_name:
            enum_name = _enum_class_name(segment.vocabulary_name)
            used_enums.add(enum_name)

    enum_imports = ",\n    ".join(sorted(used_enums))
    return dedent(
        f"""
from __future__ import annotations

from collections.abc import Mapping
from dataclasses import dataclass

from .model_types import (
    {enum_imports},
)

        """
    ).strip()


def _import_block() -> str:
    return dedent(
        """
from __future__ import annotations

from collections.abc import Mapping
from dataclasses import dataclass
from enum import StrEnum
        """
    ).strip()


def _segment_rule_dataclass() -> str:
    return dedent(
        """
        @dataclass(frozen=True)
        class SegmentRule:
            identifier: str
            optional: bool
            template: str | None
            exclusive_with: tuple[str, ...]
            tokens: tuple[str, ...]

        """
    ).strip()


def _enum_definitions(spec: Any) -> str:
    blocks: list[str] = []
    for vocab_name, tokens in spec.vocabularies.items():
        enum_name = _enum_class_name(vocab_name)
        lines = [f"class {enum_name}(StrEnum):"]
        if not tokens:
            lines.append("    pass")
        else:
            for token in tokens:
                member = _enum_member_name(token)
                # Handle long lines by wrapping if needed
                assignment = f'    {member} = "{token}"'
                if len(assignment) > 88:  # ruff's line length limit
                    lines.append(f"    {member} = (")
                    lines.append(f'        "{token}"')
                    lines.append("    )")
                else:
                    lines.append(assignment)

        # Add blank line after each enum class
        lines.append("")
        blocks.append("\n".join(lines))

    # Join with double newlines to ensure proper spacing
    result = "\n\n".join(blocks)
    return result


def _segment_metadata(spec: Any) -> dict[str, Any]:
    segment_rules: list[dict[str, Any]] = []
    for segment in spec.segments:
        segment_rules.append(
            {
                "identifier": segment.identifier,
                "optional": segment.optional,
                "template": segment.template,
                "exclusive_with": tuple(segment.exclusive_with),
            }
        )

    segment_order = tuple(segment.identifier for segment in spec.segments)

    segment_templates = {
        segment.identifier: segment.template
        for segment in spec.segments
        if segment.template
    }

    segment_token_map = {
        segment.identifier: tuple(spec.tokens_for_segment(segment.identifier))
        for segment in spec.segments
    }

    segment_enum_map = {
        segment.identifier: (
            _enum_class_name(segment.vocabulary_name)
            if segment.vocabulary_name
            else None
        )
        for segment in spec.segments
    }

    exclusive_pairs = sorted(
        {
            tuple(sorted((segment.identifier, other)))
            for segment in spec.segments
            for other in segment.exclusive_with
        }
    )

    return {
        "segment_rules": tuple(segment_rules),
        "segment_order": segment_order,
        "segment_templates": segment_templates,
        "segment_token_map": segment_token_map,
        "segment_enum_map": segment_enum_map,
        "exclusive_pairs": tuple(tuple(pair) for pair in exclusive_pairs),
    }


def _render_segment_metadata(meta: Mapping[str, Any]) -> str:
    token_map_lines: list[str] = ["SEGMENT_TOKEN_MAP: dict[str, tuple[str, ...]] = {"]
    segment_enum_map: Mapping[str, str | None] = meta["segment_enum_map"]
    for identifier, tokens in meta["segment_token_map"].items():
        enum_name = segment_enum_map.get(identifier)
        if enum_name:
            value_expr = f"tuple(member.value for member in {enum_name})"
            token_map_lines.append(f'    "{identifier}": {value_expr},')
            continue

        if tokens:
            tuple_repr = _format_tuple_literal(tokens, indent=8, base_indent=4)
            token_map_lines.append(f'    "{identifier}": {tuple_repr},')
        else:
            token_map_lines.append(f'    "{identifier}": (),')
    token_map_lines.append("}")
    token_map_repr = "\n".join(token_map_lines)
    order_repr = _format_tuple_literal(meta["segment_order"], indent=4, base_indent=0)
    template_map_repr = _format_str_dict_literal(meta["segment_templates"])
    exclusive_repr = _format_tuple_literal(
        meta["exclusive_pairs"], indent=4, base_indent=0
    )

    rule_blocks: list[str] = []
    for entry in meta["segment_rules"]:
        template_repr = (
            repr(entry["template"]) if entry["template"] is not None else "None"
        )
        block = dedent(
            f"""
            SegmentRule(
                identifier="{entry["identifier"]}",
                optional={entry["optional"]},
                template={template_repr},
                exclusive_with={entry["exclusive_with"]},
                tokens=SEGMENT_TOKEN_MAP["{entry["identifier"]}"],
            ),

            """
        ).strip()
        rule_blocks.append(indent(block, "    "))

    rules_section = "\n".join(rule_blocks)

    # Find base segment indices (geometric_base or physical_base)
    # These are mutually exclusive and mark the boundary between prefix and suffix segments
    base_indices = [
        i
        for i, seg in enumerate(meta["segment_order"])
        if seg in ("geometric_base", "physical_base", "base")
    ]
    if not base_indices:
        raise ValueError(
            "Grammar must have at least one base segment (geometric_base, physical_base, or base)"
        )

    prefix_suffix_block = dedent(
        f"""
        # Base segments are at indices {base_indices} in SEGMENT_ORDER
        # They mark the boundary between prefix (component, coordinate, subject) and suffix (object, geometry, position, process) segments
        BASE_SEGMENT_INDICES: tuple[int, ...] = {tuple(base_indices)}
        BASE_SEGMENTS: tuple[str, ...] = {tuple(meta["segment_order"][i] for i in base_indices)}
        PREFIX_SEGMENTS: tuple[str, ...] = SEGMENT_ORDER[: BASE_SEGMENT_INDICES[0]]
        SUFFIX_SEGMENTS: tuple[str, ...] = SEGMENT_ORDER[BASE_SEGMENT_INDICES[-1] + 1 :]
        SUFFIX_SEGMENTS_REVERSED: tuple[str, ...] = tuple(reversed(SUFFIX_SEGMENTS))

        """
    ).strip()

    token_search_block = dedent(
        """
        SEGMENT_SEARCH_TOKEN_MAP: dict[str, tuple[str, ...]] = {
            key: tuple(sorted(tokens, key=len, reverse=True))
            for key, tokens in SEGMENT_TOKEN_MAP.items()
        }
        SEGMENT_PREFIX_TOKEN_MAP: Mapping[str, tuple[str, ...]] = SEGMENT_SEARCH_TOKEN_MAP
        SEGMENT_SUFFIX_TOKEN_MAP: Mapping[str, tuple[str, ...]] = SEGMENT_SEARCH_TOKEN_MAP

        """
    ).strip()

    sections = [
        token_map_repr,
        dedent(
            """
            SEGMENT_RULES: tuple[SegmentRule, ...] = (
            {rules_block}
            )
            """
        )
        .format(rules_block=rules_section)
        .strip(),
        f"SEGMENT_ORDER: tuple[str, ...] = {order_repr}",
        prefix_suffix_block,
        f"SEGMENT_TEMPLATES: dict[str, str] = {template_map_repr}",
        token_search_block,
        f"EXCLUSIVE_SEGMENT_PAIRS: tuple[tuple[str, str], ...] = {exclusive_repr}",
    ]

    return "\n\n".join(section for section in sections if section)


def _render_scope_metadata(spec: Any) -> str:
    """Render applicability metadata (when to create standard names)."""
    sections = []

    # Try 'applicability' first (new name), fall back to 'scope' (legacy)
    applicability = getattr(spec, "applicability", None) or getattr(spec, "scope", None)

    if applicability:
        include_repr = _format_tuple_literal(
            applicability.include, indent=4, base_indent=0
        )
        exclude_repr = _format_tuple_literal(
            applicability.exclude, indent=4, base_indent=0
        )
        rationale_repr = repr(applicability.rationale)

        applicability_section = (
            "# Applicability: when to create standard names\n"
            f"APPLICABILITY_INCLUDE: tuple[str, ...] = {include_repr}\n"
            f"APPLICABILITY_EXCLUDE: tuple[str, ...] = {exclude_repr}\n"
            f"APPLICABILITY_RATIONALE: str = {rationale_repr}"
        )
        sections.append(applicability_section)

    # Add generic physical bases constant if the vocabulary exists
    if "generic_physical_bases" in spec.vocabularies:
        generic_bases = spec.vocabularies["generic_physical_bases"]
        generic_repr = _format_tuple_literal(generic_bases, indent=4, base_indent=0)
        generic_section = (
            "\n# Generic physical bases that require qualification\n"
            "# These physical_base tokens cannot stand alone and must have\n"
            "# subject, device, object, position, or geometry qualification\n"
            f"GENERIC_PHYSICAL_BASES: tuple[str, ...] = {generic_repr}"
        )
        sections.append(generic_section)

    return "\n\n".join(sections) if sections else ""


def _types_export_block(spec: Any) -> str:
    names = [_enum_class_name(name) for name in spec.vocabularies]
    formatted = ",\n    ".join(f'"{name}"' for name in names)
    return "\n\n__all__ = [\n    " + formatted + ",\n]"


def _constants_export_block() -> str:
    constants = [
        "SegmentRule",
        "SEGMENT_TOKEN_MAP",
        "SEGMENT_RULES",
        "SEGMENT_ORDER",
        "BASE_SEGMENT_INDICES",
        "BASE_SEGMENTS",
        "PREFIX_SEGMENTS",
        "SUFFIX_SEGMENTS",
        "SUFFIX_SEGMENTS_REVERSED",
        "SEGMENT_TEMPLATES",
        "SEGMENT_SEARCH_TOKEN_MAP",
        "SEGMENT_PREFIX_TOKEN_MAP",
        "SEGMENT_SUFFIX_TOKEN_MAP",
        "EXCLUSIVE_SEGMENT_PAIRS",
        "APPLICABILITY_INCLUDE",
        "APPLICABILITY_EXCLUDE",
        "APPLICABILITY_RATIONALE",
        "GENERIC_PHYSICAL_BASES",
    ]
    formatted = ",\n    ".join(f'"{name}"' for name in constants)
    return "__all__ = [\n    " + formatted + ",\n]"


def _export_block(spec: Any) -> str:
    names = [_enum_class_name(name) for name in spec.vocabularies]
    formatted = ",\n    ".join(f"'{name}'" for name in names)
    return "__all__ = [\n    " + formatted + "\n]"


def _format_tuple_literal(
    values: list[Any] | tuple[Any, ...], indent: int, base_indent: int
) -> str:
    items = list(values)
    if not items:
        return "()"
    element_indent = " " * indent
    closing_indent = " " * base_indent
    lines = [f"{element_indent}{repr(item)}," for item in items]
    return "(\n" + "\n".join(lines) + f"\n{closing_indent})"


def _format_str_dict_literal(mapping: Mapping[str, str]) -> str:
    if not mapping:
        return "{}"
    lines = ["{"]
    for key, value in mapping.items():
        lines.append(f'    "{key}": "{value}",')
    lines.append("}")
    return "\n".join(lines)


def _enum_member_name(token: str) -> str:
    return token.upper()


def _pascal_case(text: str) -> str:
    return "".join(part.capitalize() for part in text.split("_"))


def _singularize(name: str) -> str:
    if name.endswith("ies"):
        return name[:-3] + "y"
    if name.endswith("ses"):
        return name[:-2]
    if name.endswith("s"):
        return name[:-1]
    return name


def _enum_class_name(vocab_name: str) -> str:
    override = ENUM_NAME_OVERRIDES.get(vocab_name)
    if override:
        return override
    return _pascal_case(_singularize(vocab_name))


# ============================================================================
# Tag Type Generation
# ============================================================================


def render_tag_module(spec: TagSpec) -> str:
    """Render the tag_types.py module from TagSpec."""
    sections = [
        _tag_module_header(),
        _tag_import_block(),
        _tag_literal_types(spec),
        _tag_metadata(spec),
        _tag_export_block(),
    ]

    parts: list[str] = []
    for section in sections:
        if not section:
            continue
        text = section.strip()
        if not text:
            continue
        if parts:
            parts.append("\n\n")
        parts.append(text)

    body = "".join(parts)
    return f"{body}\n"


def _tag_module_header() -> str:
    return f'"""{TAG_HEADER.strip()}"""'


def _tag_import_block() -> str:
    return dedent(
        """
        from __future__ import annotations

        from typing import Literal
        """
    ).strip()


def _tag_literal_types(spec: TagSpec) -> str:
    """Generate Literal type aliases for primary and secondary tags."""
    primary_values = ", ".join(f'"{tag}"' for tag in spec.primary_tags)
    secondary_values = ", ".join(f'"{tag}"' for tag in spec.secondary_tags)

    return dedent(
        f"""
        # Primary tags define catalog subdirectory organization (tags[0])
        PrimaryTag = Literal[{primary_values}]

        # Secondary tags provide cross-cutting classification (tags[1:])
        SecondaryTag = Literal[{secondary_values}]

        # Union type for any tag
        Tag = PrimaryTag | SecondaryTag
        """
    ).strip()


def _tag_metadata(spec: TagSpec) -> str:
    """Generate metadata constants for tag descriptions and examples."""
    # Primary tag descriptions
    primary_desc_lines = ["PRIMARY_TAG_DESCRIPTIONS: dict[str, str] = {"]
    for tag_id in spec.primary_tags:
        meta = spec.primary_metadata.get(tag_id, {})
        description = meta.get("description", "")
        primary_desc_lines.append(f"    '{tag_id}': '{description}',")
    primary_desc_lines.append("}")
    primary_desc_block = "\n".join(primary_desc_lines)

    # Secondary tag descriptions
    secondary_desc_lines = ["SECONDARY_TAG_DESCRIPTIONS: dict[str, str] = {"]
    for tag_id in spec.secondary_tags:
        meta = spec.secondary_metadata.get(tag_id, {})
        description = meta.get("description", "")
        secondary_desc_lines.append(f"    '{tag_id}': '{description}',")
    secondary_desc_lines.append("}")
    secondary_desc_block = "\n".join(secondary_desc_lines)

    # All tags tuple for validation
    primary_tuple = _format_tuple_literal(spec.primary_tags, indent=4, base_indent=0)
    secondary_tuple = _format_tuple_literal(
        spec.secondary_tags, indent=4, base_indent=0
    )

    return (
        f"{primary_desc_block}\n\n"
        f"{secondary_desc_block}\n\n"
        f"PRIMARY_TAGS: tuple[str, ...] = {primary_tuple}\n\n"
        f"SECONDARY_TAGS: tuple[str, ...] = {secondary_tuple}"
    )


def _tag_export_block() -> str:
    return dedent(
        """
        __all__ = [
            'PrimaryTag',
            'SecondaryTag',
            'Tag',
            'PRIMARY_TAGS',
            'SECONDARY_TAGS',
            'PRIMARY_TAG_DESCRIPTIONS',
            'SECONDARY_TAG_DESCRIPTIONS',
        ]
        """
    ).strip()


def render_field_schemas_module(spec: EntrySchemaSpec) -> str:
    """Render the field_schemas.py module from EntrySchemaSpec."""
    sections = [
        _field_schemas_module_header(),
        _field_schemas_import_block(),
        _naming_guidance(spec),
        _documentation_guidance(spec),
        _field_descriptions(spec),
        _field_constraints(spec),
        _field_guidance(spec),
        _type_specific_requirements(spec),
        _provenance_modes_info(spec),
        _kind_models_mapping(spec),
        _field_schemas_export_block(),
    ]

    parts: list[str] = []
    for section in sections:
        if not section:
            continue
        text = section.strip()
        if not text:
            continue
        if parts:
            parts.append("\n\n")
        parts.append(text)

    body = "".join(parts)
    return f"{body}\n"


def _field_schemas_module_header() -> str:
    return f'"""{FIELD_SCHEMAS_HEADER.strip()}"""'


def _field_schemas_import_block() -> str:
    return dedent(
        """
        from __future__ import annotations

        from typing import Any
        """
    ).strip()


def _naming_guidance(spec: EntrySchemaSpec) -> str:
    """Generate NAMING_GUIDANCE from specification.yml naming_guidance section."""
    # Use file path instead of package import to avoid circular dependency
    spec_path = _GRAMMAR_DIR / "specification.yml"

    with open(spec_path, encoding="utf-8") as handle:
        data = yaml.load(handle, Loader=IncludeLoader) or {}

    naming_guidance = data.get("naming_guidance", {})
    if not naming_guidance:
        return "NAMING_GUIDANCE: dict[str, Any] = {}"

    guidance_repr = _format_dict_literal(naming_guidance, indent=4)
    return f"NAMING_GUIDANCE: dict[str, Any] = {guidance_repr}"


def _documentation_guidance(spec: EntrySchemaSpec) -> str:
    """Generate DOCUMENTATION_GUIDANCE from documentation field guidance."""
    # Extract documentation guidance from the documentation field
    doc_field = next((f for f in spec.fields if f.name == "documentation"), None)
    if not doc_field or not doc_field.guidance:
        return "DOCUMENTATION_GUIDANCE: dict[str, Any] = {}"

    guidance_repr = _format_dict_literal(doc_field.guidance, indent=4)
    return f"DOCUMENTATION_GUIDANCE: dict[str, Any] = {guidance_repr}"


def _field_descriptions(spec: EntrySchemaSpec) -> str:
    """Generate FIELD_DESCRIPTIONS dictionary from field specs."""
    lines = ["FIELD_DESCRIPTIONS: dict[str, str] = {"]
    for field in spec.fields:
        # Escape quotes in description
        desc = field.description.replace('"', '\\"')
        lines.append(f'    "{field.name}": "{desc}",')
    lines.append("}")
    return "\n".join(lines)


def _field_constraints(spec: EntrySchemaSpec) -> str:
    """Generate FIELD_CONSTRAINTS dictionary from field specs."""
    lines = ["FIELD_CONSTRAINTS: dict[str, dict[str, Any]] = {"]
    for field in spec.fields:
        constraints: dict[str, Any] = {
            "required": field.required,
            "type": field.field_type,
        }
        if field.pattern:
            constraints["pattern"] = field.pattern
        if field.max_length:
            constraints["max_length"] = field.max_length
        if field.examples:
            # Convert examples to JSON-serializable format
            examples_list = []
            for ex in field.examples:
                if isinstance(ex, (dict, list)):
                    examples_list.append(ex)
                else:
                    examples_list.append(str(ex))
            constraints["examples"] = examples_list

        # Format as Python dict literal
        constraints_repr = _format_dict_literal(constraints, indent=8)
        lines.append(f'    "{field.name}": {constraints_repr},')
    lines.append("}")
    return "\n".join(lines)


def _field_guidance(spec: EntrySchemaSpec) -> str:
    """Generate FIELD_GUIDANCE dictionary from field specs.

    Skips fields with empty guidance since they provide no value.
    """
    lines = ["FIELD_GUIDANCE: dict[str, dict[str, Any]] = {"]
    for field in spec.fields:
        # Skip fields with no guidance (empty dict)
        if not field.guidance:
            continue
        guidance_repr = _format_dict_literal(field.guidance, indent=8)
        lines.append(f'    "{field.name}": {guidance_repr},')
    lines.append("}")
    return "\n".join(lines)


def _type_specific_requirements(spec: EntrySchemaSpec) -> str:
    """Generate TYPE_SPECIFIC_REQUIREMENTS dictionary."""
    lines = ["TYPE_SPECIFIC_REQUIREMENTS: dict[str, dict[str, Any]] = {"]
    for kind, requirements in spec.type_specific.items():
        req_repr = _format_dict_literal(requirements, indent=8)
        lines.append(f'    "{kind}": {req_repr},')
    lines.append("}")
    return "\n".join(lines)


def _provenance_modes_info(spec: EntrySchemaSpec) -> str:
    """Generate PROVENANCE_MODES_INFO constant from specification."""
    if not spec.provenance_modes_info:
        return "PROVENANCE_MODES_INFO: dict[str, Any] | None = None"

    info_repr = _format_dict_literal(spec.provenance_modes_info, indent=4)
    return f"PROVENANCE_MODES_INFO: dict[str, Any] | None = {info_repr}"


def _kind_models_mapping(spec: EntrySchemaSpec) -> str:
    """Generate KIND_MODELS mapping of Kind enum to model classes.

    This provides a programmatic mapping that can be used by tools
    instead of hardcoding kind-to-model mappings.
    """
    lines = [
        "# Mapping of Kind enum values to Pydantic model classes",
        "# Import these in your code to avoid hardcoding kind-to-model mappings",
        "try:",
        "    from imas_standard_names.models import (",
        "        Kind,",
        "        StandardNameScalarEntry,",
        "        StandardNameVectorEntry,",
        "        StandardNameMetadataEntry,",
        "    )",
        "",
        "    KIND_MODELS: dict[Kind, type] = {",
        "        Kind.scalar: StandardNameScalarEntry,",
        "        Kind.vector: StandardNameVectorEntry,",
        "        Kind.metadata: StandardNameMetadataEntry,",
        "    }",
        "except ImportError:",
        "    # Models not yet available during initial codegen",
        "    KIND_MODELS = {}  # type: ignore[assignment]",
    ]
    return "\n".join(lines)


def _format_dict_literal(d: dict[str, Any], indent: int) -> str:
    """Format a dictionary as a Python literal with proper indentation."""
    if not d:
        return "{}"

    lines = ["{"]
    indent_str = " " * indent
    for key, value in d.items():
        value_repr = _format_value(value, indent + 4)
        lines.append(f'{indent_str}"{key}": {value_repr},')
    lines.append(" " * (indent - 4) + "}")
    return "\n".join(lines)


def _format_value(value: Any, indent: int) -> str:
    """Format a value for Python literal representation."""
    if isinstance(value, str):
        # Escape quotes and newlines
        escaped = value.replace("\\", "\\\\").replace('"', '\\"').replace("\n", "\\n")
        return f'"{escaped}"'
    elif isinstance(value, bool):
        return str(value)
    elif isinstance(value, (int, float)):
        return str(value)
    elif isinstance(value, list):
        if not value:
            return "[]"
        if all(isinstance(v, str) for v in value):
            # Escape backslashes in each string item
            escaped_items = [v.replace("\\", "\\\\").replace('"', '\\"') for v in value]
            items = ", ".join(f'"{item}"' for item in escaped_items)
            return f"[{items}]"
        else:
            # Multi-line list for complex items
            indent_str = " " * indent
            items = [f"{indent_str}{_format_value(v, indent + 4)}" for v in value]
            return "[\n" + ",\n".join(items) + "\n" + " " * (indent - 4) + "]"
    elif isinstance(value, dict):
        return _format_dict_literal(value, indent + 4)
    else:
        return repr(value)


def _field_schemas_export_block() -> str:
    return dedent(
        """
        __all__ = [
            'NAMING_GUIDANCE',
            'DOCUMENTATION_GUIDANCE',
            'FIELD_DESCRIPTIONS',
            'FIELD_CONSTRAINTS',
            'FIELD_GUIDANCE',
            'TYPE_SPECIFIC_REQUIREMENTS',
            'PROVENANCE_MODES_INFO',
            'KIND_MODELS',
        ]
        """
    ).strip()


if __name__ == "__main__":
    main()
