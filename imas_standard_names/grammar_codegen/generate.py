"""Generate code (types + metadata) from ``grammar.yml``.

Output: ``imas_standard_names/grammar/types.py``

Lives outside the ``grammar`` package to avoid importing any module that
depends on the generated file during generation.
"""

from __future__ import annotations

from collections.abc import Mapping
from pathlib import Path
from textwrap import dedent, indent
from typing import Any

from imas_standard_names.grammar_codegen.spec import GrammarSpec

HEADER = (
    "Auto-generated grammar models.\n\n"
    "This file was generated by "
    "imas_standard_names.grammar_codegen.generate\n"
    "from imas_standard_names/resources/grammar.yml. Do not edit manually."
)

# The output is next to the runtime grammar modules
OUTPUT_MODULE = Path(__file__).resolve().parents[1] / "grammar" / "types.py"

ENUM_NAME_OVERRIDES = {
    "components": "Component",
    "subjects": "Subject",
    "basis": "Basis",
    "positions": "Position",
    "processes": "Process",
}


def main() -> None:
    spec = GrammarSpec.load()
    content = render_module(spec)
    existing = (
        OUTPUT_MODULE.read_text(encoding="utf-8") if OUTPUT_MODULE.exists() else ""
    )
    if existing == content:
        print("grammar/types.py already up to date")
        return
    OUTPUT_MODULE.parent.mkdir(parents=True, exist_ok=True)
    OUTPUT_MODULE.write_text(content, encoding="utf-8")
    print("Updated grammar/types.py")


def render_module(spec: Any) -> str:
    metadata = _segment_metadata(spec)
    sections = [
        _module_header(),
        _import_block(),
        _enum_definitions(spec),
        _segment_rule_dataclass(),
        _render_segment_metadata(metadata),
        _export_block(spec),
    ]

    parts: list[str] = []
    previous_index: int | None = None
    for idx, section in enumerate(sections):
        if not section:
            continue
        text = section.strip()
        if not text:
            continue
        if parts:
            separator = "\n\n"
            if previous_index == 1:
                separator = "\n\n\n"
            parts.append(separator)
        parts.append(text)
        previous_index = idx

    body = "".join(parts)
    return f"{body}\n"


def _module_header() -> str:
    return f'"""{HEADER.strip()}"""'


def _import_block() -> str:
    return dedent(
        """
from __future__ import annotations

from collections.abc import Mapping
from dataclasses import dataclass
from enum import StrEnum
        """
    ).strip()


def _segment_rule_dataclass() -> str:
    return dedent(
        """
        @dataclass(frozen=True)
        class SegmentRule:
            identifier: str
            optional: bool
            template: str | None
            exclusive_with: tuple[str, ...]
            tokens: tuple[str, ...]
        """
    ).strip()


def _enum_definitions(spec: Any) -> str:
    blocks: list[str] = []
    for vocab_name, tokens in spec.vocabularies.items():
        enum_name = _enum_class_name(vocab_name)
        lines = [f"class {enum_name}(StrEnum):"]
        if not tokens:
            lines.append("    pass")
        else:
            for token in tokens:
                member = _enum_member_name(token)
                lines.append(f'    {member} = "{token}"')
        blocks.append("\n".join(lines))
    return "\n\n".join(blocks)


def _segment_metadata(spec: Any) -> dict[str, Any]:
    segment_rules: list[dict[str, Any]] = []
    for segment in spec.segments:
        segment_rules.append(
            {
                "identifier": segment.identifier,
                "optional": segment.optional,
                "template": segment.template,
                "exclusive_with": tuple(segment.exclusive_with),
            }
        )

    segment_order = tuple(segment.identifier for segment in spec.segments)

    segment_templates = {
        segment.identifier: segment.template
        for segment in spec.segments
        if segment.template
    }

    segment_token_map = {
        segment.identifier: tuple(spec.tokens_for_segment(segment.identifier))
        for segment in spec.segments
    }

    segment_enum_map = {
        segment.identifier: (
            _enum_class_name(segment.vocabulary_name)
            if segment.vocabulary_name
            else None
        )
        for segment in spec.segments
    }

    exclusive_pairs = sorted(
        {
            tuple(sorted((segment.identifier, other)))
            for segment in spec.segments
            for other in segment.exclusive_with
        }
    )

    return {
        "segment_rules": tuple(segment_rules),
        "segment_order": segment_order,
        "segment_templates": segment_templates,
        "segment_token_map": segment_token_map,
        "segment_enum_map": segment_enum_map,
        "exclusive_pairs": tuple(tuple(pair) for pair in exclusive_pairs),
    }


def _render_segment_metadata(meta: Mapping[str, Any]) -> str:
    token_map_lines: list[str] = ["SEGMENT_TOKEN_MAP: dict[str, tuple[str, ...]] = {"]
    segment_enum_map: Mapping[str, str | None] = meta["segment_enum_map"]
    for identifier, tokens in meta["segment_token_map"].items():
        enum_name = segment_enum_map.get(identifier)
        if enum_name:
            value_expr = f"tuple(member.value for member in {enum_name})"
            token_map_lines.append(f"    '{identifier}': {value_expr},")
            continue

        if tokens:
            tuple_repr = _format_tuple_literal(tokens, indent=8, base_indent=4)
            token_map_lines.append(f"    '{identifier}': {tuple_repr},")
        else:
            token_map_lines.append(f"    '{identifier}': (),")
    token_map_lines.append("}")
    token_map_repr = "\n".join(token_map_lines)
    order_repr = _format_tuple_literal(meta["segment_order"], indent=4, base_indent=0)
    template_map_repr = _format_str_dict_literal(meta["segment_templates"])
    exclusive_repr = _format_tuple_literal(
        meta["exclusive_pairs"], indent=4, base_indent=0
    )

    rule_blocks: list[str] = []
    for entry in meta["segment_rules"]:
        template_repr = (
            repr(entry["template"]) if entry["template"] is not None else "None"
        )
        block = dedent(
            f"""
            SegmentRule(
                identifier='{entry["identifier"]}',
                optional={entry["optional"]},
                template={template_repr},
                exclusive_with={entry["exclusive_with"]},
                tokens=SEGMENT_TOKEN_MAP['{entry["identifier"]}'],
            ),
            """
        ).strip()
        rule_blocks.append(indent(block, "    "))

    rules_section = "\n".join(rule_blocks)
    prefix_suffix_block = dedent(
        """
        _PREFIX_END = SEGMENT_ORDER.index('base')
        PREFIX_SEGMENTS: tuple[str, ...] = SEGMENT_ORDER[:_PREFIX_END]
        SUFFIX_SEGMENTS: tuple[str, ...] = SEGMENT_ORDER[_PREFIX_END + 1 :]
        SUFFIX_SEGMENTS_REVERSED: tuple[str, ...] = tuple(reversed(SUFFIX_SEGMENTS))
        """
    ).strip()

    token_search_block = dedent(
        """
        SEGMENT_SEARCH_TOKEN_MAP: dict[str, tuple[str, ...]] = {
            key: tuple(sorted(tokens, key=len, reverse=True))
            for key, tokens in SEGMENT_TOKEN_MAP.items()
        }
        SEGMENT_PREFIX_TOKEN_MAP: Mapping[str, tuple[str, ...]] = SEGMENT_SEARCH_TOKEN_MAP
        SEGMENT_SUFFIX_TOKEN_MAP: Mapping[str, tuple[str, ...]] = SEGMENT_SEARCH_TOKEN_MAP
        """
    ).strip()

    sections = [
        token_map_repr,
        dedent(
            """
            SEGMENT_RULES: tuple[SegmentRule, ...] = (
            {rules_block}
            )
            """
        )
        .format(rules_block=rules_section)
        .strip(),
        f"SEGMENT_ORDER: tuple[str, ...] = {order_repr}",
        prefix_suffix_block,
        f"SEGMENT_TEMPLATES: dict[str, str] = {template_map_repr}",
        token_search_block,
        f"EXCLUSIVE_SEGMENT_PAIRS: tuple[tuple[str, str], ...] = {exclusive_repr}",
    ]

    return "\n\n".join(section for section in sections if section)


def _export_block(spec: Any) -> str:
    names = [_enum_class_name(name) for name in spec.vocabularies]
    formatted = ",\n    ".join(f"'{name}'" for name in names)
    return "__all__ = [\n    " + formatted + "\n]"


def _format_tuple_literal(
    values: list[Any] | tuple[Any, ...], indent: int, base_indent: int
) -> str:
    items = list(values)
    if not items:
        return "()"
    element_indent = " " * indent
    closing_indent = " " * base_indent
    lines = [f"{element_indent}{repr(item)}," for item in items]
    return "(\n" + "\n".join(lines) + f"\n{closing_indent})"


def _format_str_dict_literal(mapping: Mapping[str, str]) -> str:
    if not mapping:
        return "{}"
    lines = ["{"]
    for key, value in mapping.items():
        lines.append(f"    '{key}': '{value}',")
    lines.append("}")
    return "\n".join(lines)


def _enum_member_name(token: str) -> str:
    return token.upper()


def _pascal_case(text: str) -> str:
    return "".join(part.capitalize() for part in text.split("_"))


def _singularize(name: str) -> str:
    if name.endswith("ies"):
        return name[:-3] + "y"
    if name.endswith("ses"):
        return name[:-2]
    if name.endswith("s"):
        return name[:-1]
    return name


def _enum_class_name(vocab_name: str) -> str:
    override = ENUM_NAME_OVERRIDES.get(vocab_name)
    if override:
        return override
    return _pascal_case(_singularize(vocab_name))


if __name__ == "__main__":
    main()
