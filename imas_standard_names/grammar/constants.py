"""Grammar constants and metadata.

This module contains all constants and metadata used by the grammar system,
separated from the types to avoid circular imports between grammar/support.py
and grammar/types.py.

Auto-generated grammar models.

This file was generated by imas_standard_names.grammar_codegen.generate
from imas_standard_names/grammar/specification.yml. Do not edit manually."""

from __future__ import annotations

from collections.abc import Mapping
from dataclasses import dataclass

from .types import (
    Component,
    GeometricBase,
    Object,
    Position,
    Process,
    Source,
    Subject,
)


@dataclass(frozen=True)
class SegmentRule:
    identifier: str
    optional: bool
    template: str | None
    exclusive_with: tuple[str, ...]
    tokens: tuple[str, ...]


SEGMENT_TOKEN_MAP: dict[str, tuple[str, ...]] = {
    "component": tuple(member.value for member in Component),
    "coordinate": tuple(member.value for member in Component),
    "subject": tuple(member.value for member in Subject),
    "geometric_base": tuple(member.value for member in GeometricBase),
    "physical_base": (),
    "object": tuple(member.value for member in Object),
    "source": tuple(member.value for member in Source),
    "geometry": tuple(member.value for member in Position),
    "position": tuple(member.value for member in Position),
    "process": tuple(member.value for member in Process),
}

SEGMENT_RULES: tuple[SegmentRule, ...] = (
    SegmentRule(
        identifier="component",
        optional=True,
        template="{token}_component_of",
        exclusive_with=("coordinate",),
        tokens=SEGMENT_TOKEN_MAP["component"],
    ),
    SegmentRule(
        identifier="coordinate",
        optional=True,
        template="{token}",
        exclusive_with=("component",),
        tokens=SEGMENT_TOKEN_MAP["coordinate"],
    ),
    SegmentRule(
        identifier="subject",
        optional=True,
        template=None,
        exclusive_with=(),
        tokens=SEGMENT_TOKEN_MAP["subject"],
    ),
    SegmentRule(
        identifier="geometric_base",
        optional=True,
        template=None,
        exclusive_with=("physical_base",),
        tokens=SEGMENT_TOKEN_MAP["geometric_base"],
    ),
    SegmentRule(
        identifier="physical_base",
        optional=True,
        template=None,
        exclusive_with=("geometric_base",),
        tokens=SEGMENT_TOKEN_MAP["physical_base"],
    ),
    SegmentRule(
        identifier="object",
        optional=True,
        template="of_{token}",
        exclusive_with=("source",),
        tokens=SEGMENT_TOKEN_MAP["object"],
    ),
    SegmentRule(
        identifier="source",
        optional=True,
        template="from_{token}",
        exclusive_with=("object",),
        tokens=SEGMENT_TOKEN_MAP["source"],
    ),
    SegmentRule(
        identifier="geometry",
        optional=True,
        template="of_{token}",
        exclusive_with=("position",),
        tokens=SEGMENT_TOKEN_MAP["geometry"],
    ),
    SegmentRule(
        identifier="position",
        optional=True,
        template="at_{token}",
        exclusive_with=("geometry",),
        tokens=SEGMENT_TOKEN_MAP["position"],
    ),
    SegmentRule(
        identifier="process",
        optional=True,
        template="due_to_{token}",
        exclusive_with=(),
        tokens=SEGMENT_TOKEN_MAP["process"],
    ),
)

SEGMENT_ORDER: tuple[str, ...] = (
    "component",
    "coordinate",
    "subject",
    "geometric_base",
    "physical_base",
    "object",
    "source",
    "geometry",
    "position",
    "process",
)

# Base segments are at indices [3, 4] in SEGMENT_ORDER
# They mark the boundary between prefix (component, coordinate, subject) and suffix (object, source, etc.) segments
BASE_SEGMENT_INDICES: tuple[int, ...] = (3, 4)
BASE_SEGMENTS: tuple[str, ...] = ("geometric_base", "physical_base")
PREFIX_SEGMENTS: tuple[str, ...] = SEGMENT_ORDER[: BASE_SEGMENT_INDICES[0]]
SUFFIX_SEGMENTS: tuple[str, ...] = SEGMENT_ORDER[BASE_SEGMENT_INDICES[-1] + 1 :]
SUFFIX_SEGMENTS_REVERSED: tuple[str, ...] = tuple(reversed(SUFFIX_SEGMENTS))

SEGMENT_TEMPLATES: dict[str, str] = {
    "component": "{token}_component_of",
    "coordinate": "{token}",
    "object": "of_{token}",
    "source": "from_{token}",
    "geometry": "of_{token}",
    "position": "at_{token}",
    "process": "due_to_{token}",
}

SEGMENT_SEARCH_TOKEN_MAP: dict[str, tuple[str, ...]] = {
    key: tuple(sorted(tokens, key=len, reverse=True))
    for key, tokens in SEGMENT_TOKEN_MAP.items()
}
SEGMENT_PREFIX_TOKEN_MAP: Mapping[str, tuple[str, ...]] = SEGMENT_SEARCH_TOKEN_MAP
SEGMENT_SUFFIX_TOKEN_MAP: Mapping[str, tuple[str, ...]] = SEGMENT_SEARCH_TOKEN_MAP

EXCLUSIVE_SEGMENT_PAIRS: tuple[tuple[str, str], ...] = (
    ("component", "coordinate"),
    ("geometric_base", "physical_base"),
    ("geometry", "position"),
    ("object", "source"),
)

# Scope: when to create standard names
SCOPE_INCLUDE: tuple[str, ...] = (
    "Physical measurements and diagnostic signals",
    "Derived quantities, calculations, and operators",
    "Hardware geometric/physical properties needed for data interpretation",
    "Coordinate variables (time, space, flux)",
)
SCOPE_EXCLUDE: tuple[str, ...] = (
    "Facility-specific identifiers and labels  (paths like */name, */identifier, */label)",
    "Type enumeration indices (paths like */type/index)",
    "Code provenance metadata (paths like */code/*, */ids_properties/*)",
)
SCOPE_RATIONALE: str = "Standard names apply to physical quantities with standardizable meaning and units,  not to administrative metadata\n"

__all__ = [
    "SegmentRule",
    "SEGMENT_TOKEN_MAP",
    "SEGMENT_RULES",
    "SEGMENT_ORDER",
    "BASE_SEGMENT_INDICES",
    "BASE_SEGMENTS",
    "PREFIX_SEGMENTS",
    "SUFFIX_SEGMENTS",
    "SUFFIX_SEGMENTS_REVERSED",
    "SEGMENT_TEMPLATES",
    "SEGMENT_SEARCH_TOKEN_MAP",
    "SEGMENT_PREFIX_TOKEN_MAP",
    "SEGMENT_SUFFIX_TOKEN_MAP",
    "EXCLUSIVE_SEGMENT_PAIRS",
    "SCOPE_INCLUDE",
    "SCOPE_EXCLUDE",
    "SCOPE_RATIONALE",
]
